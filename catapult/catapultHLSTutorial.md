

# Catapult HLS for ASICs: A Comprehensive Tutorial

## Introduction

High-Level Synthesis (HLS) allows designers to generate hardware from high-level languages, dramatically raising the abstraction above RTL design. **Catapult HLS**, a tool from Siemens EDA (formerly Mentor Graphics), takes C++ or SystemC descriptions and produces optimized Register-Transfer Level (RTL) code for hardware implementation. Unlike traditional RTL coding, HLS lets you describe functionality in a more algorithmic form and rely on the tool to handle timing and resource allocation. This tutorial provides a holistic guide to using Catapult HLS for **ASIC** design – from basic concepts to advanced optimizations – with an emphasis on ASIC considerations (like timing closure, power, and physical design) over FPGA-specific concerns. We will cover the HLS design methodology (behavioral modeling, interface design, clock and timing constraints, pipelining, memory optimizations, verification) and explain key microarchitectural transformations (loop unrolling, pipelining, function inlining, resource sharing, scheduling, and binding). We also highlight new features in recent Catapult HLS versions relevant to ASIC flows, and illustrate concepts with annotated code examples and real-world case studies.

HLS has matured to the point that major companies use it for production ASICs. For example, NVIDIA reported that adopting a C++ HLS flow with Catapult simplified their code by 5× and enabled 1000× more tests (drastically reducing verification time), cutting overall design time in half. With modern HLS tools like Catapult, designers can achieve competitive power, performance, and area (PPA) on ASICs while significantly boosting productivity and design exploration capability.

## HLS Design Methodology Overview

Designing hardware with Catapult HLS involves several stages, from writing a high-level model to verifying the generated RTL. Below is a typical **HLS design flow** for an ASIC target:

1. **Write a High-Level Model:** Develop the algorithmic behavior in C++ or SystemC (untimed or lightly timed code), using bit-accurate data types where needed (e.g. fixed-point types) to match hardware precision.
2. **High-Level Simulation:** Simulate and debug the C++/SystemC model to ensure functional correctness before synthesis. This may involve creating a high-level testbench that feeds input vectors and checks outputs.
3. **Apply HLS Constraints and Synthesis:** Import the code into Catapult HLS. Specify target technology (ASIC library), clock frequency, and interface protocols, then run high-level synthesis to generate RTL. The tool will schedule operations into clock cycles and perform binding of operations to hardware units based on the given constraints.
4. **Review and Optimize Microarchitecture:** Analyze HLS results (latency, throughput, area estimates). If the design doesn’t meet performance or area goals, iterate by adding HLS directives or refining the code. Common optimizations include loop pipelining, loop unrolling, function inlining, and adjusting resource allocation. Each iteration, re-synthesize and review the updated design metrics.
5. **RTL Verification:** Once the HLS-generated RTL is satisfactory, verify it thoroughly. This can include **co-simulation** (running the original C++ testbench against the RTL output), formal verification (to prove equivalence between C++ model and RTL), and checking coverage. Ensure the RTL meets timing (perform static timing analysis with the ASIC library) and is ready for integration.
6. **Integration and Backend:** Integrate the RTL into the larger SoC or ASIC design, then proceed with logic synthesis, place-and-route, and physical implementation. Because Catapult can target both FPGAs and ASICs, it produces technology-independent RTL. For ASIC, you will use a standard cell library (e.g., via Synopsys Design Compiler or similar) to map the RTL to gates and then perform place-and-route. After layout, perform final timing verification and signoff checks before tapeout.

Each of these steps involves specific HLS concepts and best practices. In the subsections below, we dive deeper into key aspects of HLS design methodology, focusing on how to model and optimize designs in Catapult HLS for ASIC targets.

### Behavioral Modeling in C++/SystemC

The first step is to write a **behavioral model** of your hardware in a high-level language. Catapult accepts standard C++ as well as SystemC (a C++ library for hardware modeling). For beginners, writing a pure C/C++ function to describe the desired behavior is a good starting point. More advanced users might use SystemC to explicitly model hardware concurrency (threads, signals, clocks), but Catapult can synthesize both styles. In either case, you must adhere to a *synthesizable subset* of the language (no infinite loops, limited or no dynamic memory allocation, etc.), similar to writing synthesizable RTL in VHDL/Verilog.

**Choosing Data Types:** Use precise data types that reflect hardware bit-width needs. Catapult supports C/C++ native types (`int`, `char`, etc.) and provides *Algorithmic C (AC)* datatypes for bit-accurate modeling. For example, `ac_int<W,S>` for fixed-width integers and `ac_fixed` for fixed-point numbers allow you to control word length and integer/fractional bits. Using such types, you can tune numeric precision to balance numerical accuracy against hardware cost. This is especially important for ASIC design – reducing bit-width can save area and power. For instance, an open-source Softmax accelerator uses `ac_fixed<16,8>` types to implement a neural network softmax function with sufficient precision, and this flexibility in precision lets designers trade off area vs performance easily during HLS. In practice, you might start with floating-point or high precision, verify correctness, then quantize to a fixed-point representation for synthesis once requirements are understood.

**Untimed vs Timed Modeling:** One advantage of HLS is that you can write *untimed* code (focused only on correct data transformation, not cycle-by-cycle behavior) and let the HLS compiler decide the scheduling. For example, you can write a nested loop algorithm in C++ exactly as you would in software; Catapult will later determine how to pipeline or parallelize it to meet timing. This untimed functional style is usually best for HLS, as it gives the tool freedom to optimize. However, SystemC also allows *timed* or cycle-approximate modeling (using `wait()`, `sc_clock`, etc.), which advanced users can leverage to incorporate specific timing or cycle constraints into the model (e.g., modeling a communication protocol or an FSM with specific cycle behavior). In Catapult, timed SystemC can be synthesized too, but the general recommendation is to keep the algorithm description as high-level as possible and use HLS directives to guide the timing, rather than hard-coding cycle delays in your code.

**Example – Simple C++ Function:** Below is a simple example of a C++ function that could be synthesized by Catapult. It computes an element-wise vector addition and thresholding (for illustration purposes):

```cpp
#include <ac_int.h>  // For arbitrary-width ints (Algorithmic C library)

// Synthesize a simple thresholded vector addition
#pragma hls_design top          // Mark this function as the top-level design
void vec_add_threshold(const ac_int<16,false> A[100], const ac_int<16,false> B[100],
                       ac_int<16,false> C[100], ac_int<16,false> threshold) {
    for (int i = 0; i < 100; ++i) {
        ac_int<17,false> sum = A[i] + B[i];   // 17-bit to hold possible carry
        if (sum > threshold) 
            C[i] = threshold;
        else 
            C[i] = (ac_int<16,false>) sum;
    }
}
```

In this code, we use `ac_int<16,false>` (16-bit unsigned) for inputs and outputs, and a slightly wider type for the sum to avoid overflow. The `#pragma hls_design top` is a Catapult-specific pragma (or macro) indicating the top-level hardware function to synthesize. The code itself is written in a straightforward way, without any explicit hardware timing. Catapult will later allow us to apply transformations like pipelining to this loop to meet our performance goals.

**Coding Guidelines:** When writing the high-level model, keep in mind a few HLS coding guidelines: avoid unbounded loops or recursion (bounded loops are fine, and the tool needs to know loop trip counts either at compile time or via constraints), avoid dynamic memory allocation or system calls (stick to static arrays or new/delete outside of performance-critical code), and be careful with pointer arithmetic or aliasing (HLS tools often need to know memory access patterns; using plain arrays or `std::array`/C-style arrays is safest). Also, break the algorithm into functions if it improves clarity – Catapult can inline them later if needed. Write the code in a clear, correct manner first; micro-optimizations will be handled by HLS or through directives. Always verify the C++ model thoroughly before trusting the synthesis result.

### Interface Synthesis and I/O

In an HLS design, the *interface* refers to how the synthesized hardware block communicates with the outside world (other modules, memories, or software). In Catapult HLS, interfaces are usually inferred from function arguments or SystemC module ports, and the tool can synthesize a variety of interface protocols suitable for ASIC integration.

If you code your design as a C++ function (as in the example above), each function argument can become a hardware port. For simple scalar arguments (like the `threshold` in the example), Catapult will create an input port of the appropriate bit-width. For array arguments (`A`, `B`, `C` in the example), Catapult needs to map these to memory interfaces. By default, it might treat them as pointers to some memory and create a simple handshake or memory port interface to read/write them. You can also direct Catapult to implement specific standard interfaces. For instance, Catapult can synthesize bus interfaces like AXI4 for memory-mapped communication or AXI4-Stream for streaming data, which is useful when integrating the block in a larger SoC or connecting to a processor. There are HLS pragmas or GUI constraints to specify interface type (e.g., “register” for simple scalar, “memory” for large arrays, or protocols like AXI if required).

**Memory Interfaces:** For ASICs, a common scenario is that your HLS block will have to fetch input data from an on-chip RAM or register file and write results back. Catapult can generate a memory controller logic if you indicate an array should be an external RAM. In an SoC context, this might be an interface to a scratchpad RAM or a DMA engine. The ESP SoC framework, for example, uses a parameterized scratchpad memory and leverages Catapult HLS to generate load/store logic to and from that local memory. By specifying the array size and port widths, the HLS tool will infer the necessary address generators and handshaking logic to sequentially read or write the array. You can control aspects like *burst length* or *chunk size* of memory transfers to optimize throughput.

**Handshake and Streaming:** If your accelerator should consume or produce a stream of data (for example, one sample per cycle when available), Catapult supports *streaming interfaces* using FIFO protocols. In SystemC, you might use `sc_fifo` or `sc_stream` constructs, or in plain C++ you might rely on Catapult’s channel libraries (such as the open-source **MatchLib** library from NVIDIA now included with Catapult). These allow you to specify that your function’s inputs/outputs are streams (with ready/valid signaling), which Catapult will synthesize into hardware FIFOs and control logic. Using streaming interfaces can enable automatic pipelining across component boundaries (backpressure, etc.), which is common in ASIC dataflow architectures.

**Control Registers:** In an ASIC accelerator context, you often need control registers for configuration (e.g., the `mac_len` or other parameters in an accelerator example). Catapult can synthesize such control registers as part of the interface (for example, as memory-mapped registers via an AXI-Lite interface or a simple register bus). Typically, one would define these as additional scalar inputs or a struct of config parameters passed to the top function. The generated RTL can then be integrated with a CPU or host that writes to these registers to configure the accelerator. In our vector addition example, `threshold` can be viewed as a control register that a processor might set, whereas arrays `A`, `B`, `C` could correspond to bulk data passed via memory or streaming.

**Clock and Reset:** Catapult will create a clock and reset for the design automatically. By default, a single clock domain is assumed. You can specify the desired clock frequency for synthesis (more on this below) and whether reset is synchronous or asynchronous. In ASIC designs, typically an *asynchronous reset* that is asserted externally and then deasserted synchronous to the clock is common. The Catapult GUI or scripts allow setting reset style; for example, one might disable synchronous reset and enable asynchronous as part of the design setup. The generated RTL will then have an input `clk` and `rst` signal and apply reset to registers as per the chosen style. If multiple clock domains are needed, a common practice is to use separate HLS modules for each domain and manage the clock crossing (via FIFO or handshaking) in RTL or at the integration level – HLS tools generally synthesize a design to one clock domain at a time.

In summary, interface synthesis in Catapult involves deciding how each input/output in your high-level model maps to hardware I/O: either direct ports, buses, or memory/stream interfaces. The tool provides automation for standard interfaces, but you should plan the interface early to ensure the generated hardware will integrate smoothly into the ASIC environment.

### Clocking and Timing Constraints

Clocking is a critical aspect of ASIC design, and HLS methodology must account for it. In Catapult HLS, you specify a target clock period or frequency for the design, and the HLS compiler will schedule operations to meet that timing. For an ASIC, you base this on the performance requirements and the speed of your technology library. For example, you might set a 200 MHz target for a design in a 90nm library as in a Catapult lab exercise. Under the hood, Catapult uses *characterization data* for the operations (like add, multiply, etc.) on that target library to know how many nanoseconds each operation takes and therefore how many can fit in one clock cycle. This library-specific timing info can come from synthesis of primitives or provided models. If Catapult is in *physically aware* mode, it might also account for estimated wire delays or multi-Vt cell trade-offs for meeting timing.

When you set up the HLS project for an ASIC, you typically choose the synthesis tool (e.g., Synopsys Design Compiler) and a cell library. In the Catapult GUI flow, this involves selecting the technology and loading its delay models. Once set, **the tool’s scheduler will attempt to schedule all operations such that no path exceeds the clock period constraint**. If an operation is too slow to fit in one cycle, Catapult will automatically schedule it to take multiple cycles (or you can indicate multi-cycle operations if needed). For example, a multiplier might have a longer delay – if the clock period is very tight, the tool could schedule the multiply to start at one cycle and finish in the next (inserting pipeline registers). Conversely, if the clock period is loose, multiple operations could be chained in one cycle. The scheduler’s goal is to meet the timing while minimizing overall latency or other objectives.

**Initiation Interval and Throughput:** In HLS, especially with pipelining (discussed later), a key concept is *Initiation Interval (II)* – the number of cycles between launching consecutive operations (e.g., loop iterations). To maximize throughput, you often want II = 1 (a new operation each clock cycle). However, clock period and resource constraints can affect this. If the clock is too fast to allow a loop iteration’s work in one cycle, Catapult might default to a longer II or more cycles per iteration. You as the designer can guide this by applying a pipeline directive to request a certain II. The tool will either succeed or report if the target II is not achievable under the given clock period and data dependencies. Thus, there is a interplay between clock frequency and microarchitectural choices: a higher clock frequency might force more pipeline stages and a deeper schedule, whereas a slightly lower frequency could allow simpler scheduling with fewer stages.

**Multi-Clock Designs:** Catapult primarily synthesizes a design with a single clock domain at a time. If your ASIC module needs multiple clocks (say a fast clock for a computational core and a slower clock for an interface), you would typically break that into two HLS modules and handle the clock domain crossing separately (for example, via asynchronous FIFOs or handshake logic that you might code or integrate manually). Some advanced flows allow multiple clocks in HLS using pragmas or by instantiating dual-clock FIFO components (possibly from libraries like MatchLib), but as a general methodology, it’s simpler to keep one clock per HLS component. Focus on meeting timing in that domain; if crossing needed, ensure the interface is compatible with a CDC (Clock Domain Crossing) strategy.

**Resets and Clock Enables:** On ASICs, using clock gating for power savings is common. Catapult’s low-power optimization (via PowerPro integration) can automatically insert clock gating logic on idle flops or memories. From a design perspective, you don’t manually code gated clocks in HLS (it’s not recommended to gate `sc_clock` in SystemC or such); instead you rely on tools to infer clock enables. Ensure your code has clear enable conditions (e.g., inside loops or conditionals) so that the tool can see when certain registers are not in use – it might then generate a gated clock or an enable signal for those registers. Also, decide if you need synchronous or asynchronous reset: synchronous resets are easier for timing but add overhead in every flop; asynchronous resets are typical in ASIC for a global reset at power-up. As mentioned, Catapult allows setting this (as in the example where synchronous reset was turned off in favor of asynchronous).

In summary, treat the clock period as a key input to HLS. Be realistic by consulting your ASIC tech library capabilities. Use pipelining and other transformations to meet the target frequency. One great benefit of HLS is you can quickly experiment with different clock targets and see how the design might change (e.g., scheduling with a 500 MHz target vs 250 MHz) to decide the optimal balance of frequency vs pipeline depth. The ability to perform rapid design-space exploration with different timing constraints is a strong advantage in ASIC design, where finding the highest feasible clock frequency for required throughput can significantly affect the rest of the chip.

### Memory Access and Dataflow Optimization

Memory access patterns often dictate performance in hardware accelerators. When writing HLS code, think about how data will be fed to and from your hardware. ASICs typically have more flexibility with memory architectures than FPGAs (you can use custom multi-ported SRAMs, wide buses, etc.), but you must explicitly manage these in your HLS design or constraints.

**On-Chip vs Off-Chip Memory:** For ASIC, assume your accelerator will use on-chip memory (embedded SRAM blocks) for working data sets, as off-chip (DRAM) access would be through a memory controller or host interface not directly in the HLS module. If your design needs to handle large arrays, you should consider whether those arrays are stored in an on-chip scratchpad that the HLS block can read from and write to. In the high-level code, large arrays could be modeled as function parameters (like our examples) or as global/static arrays. Catapult can map these to either a bundle of registers (if small) or to inferred memory. In ASIC flow, typically if an array is beyond a certain size, the logic synthesis tool (Design Compiler, etc.) will infer an SRAM or you might manually replace it with a memory macro instantiation. Catapult itself can also instantiate a memory if guided (some flows use memory generator IPs). For the HLS design, you might not need to worry about the exact memory macro at first – just ensure your memory interface (read/write access pattern) is efficient.

**Memory Bandwidth and Parallelism:** A common HLS optimization is *array partitioning* or *memory banking*. If your algorithm needs multiple data elements per cycle, a single memory port could become a bottleneck. For example, if in one loop iteration you must read two array elements, and those reads happen in the same cycle in the schedule, a single-port memory would conflict. Catapult might automatically serialize those accesses unless you take action. One action is to use dual-port memories (many ASIC SRAMs support 2 ports), which Catapult can exploit. Another is to *partition* the array into multiple smaller arrays (banks) so that, for instance, all even indices reside in one bank and odd indices in another – then two reads to index i and i+1 can go to different banks and occur in parallel. HLS pragmas exist to tell the tool to split an array into N separate banks. By partitioning, you increase memory area slightly (duplicated address logic, or sometimes duplication of content for complete partition) but enable parallel accesses, increasing throughput. This is analogous to what FPGA HLS users do to get around block RAM port limits, and it equally applies to ASIC if you need >2 concurrent accesses.

Another technique is *loop interchange or tiling* to improve memory access patterns. Ensure that your inner loops access memory in a sequential (linear) pattern if possible, to take advantage of burst reads/writes. Catapult can sometimes detect bursts and coalesce sequential accesses into wide transfers. For instance, if you have a loop reading an array element by element, the tool might generate a state machine to burst-read a whole block from memory if you allow it (especially if using an AXI bus interface). You can also explicitly use burst read/write intrinsics or the streaming interfaces to feed data continuously rather than one word at a time.

**Example – Optimizing Memory in an FIR filter:** Consider a simple FIR filter that reads input samples and coefficients and produces output samples. If written naively, it might read the coefficient array from memory for every output sample, leading to repeated memory accesses for the same coefficients. A smart optimization is to load the coefficients into a local array (implemented as registers or a small ROM in hardware) once, then reuse them for every calculation. In HLS, this could be as simple as declaring the coefficient array as a static `const` array (which encourages the tool to treat it as ROM) or loading it into a local array outside the output loop. This way, coefficient memory reads are not in the inner loop, reducing memory bandwidth demands. In ASIC design, it’s usually beneficial to store constant or frequently reused data on-chip close to the compute units to avoid expensive fetches. HLS makes it easy to express this – just structure your code to perform setup outside of time-critical loops.

**Dataflow Pipelines:** Catapult HLS supports a high-level *dataflow* optimization where independent sub-tasks can run in parallel, passing data via FIFO channels. This goes beyond basic loop pipelining and is akin to building a pipeline of multiple functions or loops. For example, you might split an algorithm into stages (Stage1 produces intermediate results that Stage2 consumes). If you indicate a dataflow region, Catapult will try to run Stage1 and Stage2 concurrently, using an internal FIFO to connect them, so that while stage2 is processing data n, stage1 can process data n+1. This is an advanced optimization to maximize throughput and is especially useful when you have producer-consumer patterns. It does require writing the code in a way that stages can run in parallel (e.g., separate loops or functions for each stage) and adding a `#pragma hls_dataflow` (if supported) around them. Dataflow can get complex to verify, but it’s very powerful for ASIC designs that demand high throughput.

In summary, to optimize memory and data movement for ASIC HLS designs: ensure you have enough memory bandwidth (partition arrays or use multiple ports), minimize redundant memory access (cache or re-use data in registers when possible), align your access patterns for bursts, and consider pipelined task-level parallelism. These optimizations will help avoid the situation where the computation is fast but the hardware sits idle waiting for data. ASIC designs often bottleneck on memory, so address it at the algorithm level with the above techniques.

### Verification and Validation

Verification is a **huge** part of ASIC development – more so than FPGA – because once fabricated, bugs are extremely costly. Catapult HLS provides multiple ways to verify that your synthesized design is functionally correct and meets requirements. The verification flow starts at the C++/SystemC level and continues down to the RTL.

**High-Level Testing:** First, test your C++/SystemC model thoroughly using software simulation. This can be done in any C++ test harness or using SystemC simulation for SystemC models. The idea is to compare the algorithm’s output against known-good results or a reference model (maybe a MATLAB/Python model if available, or even a golden software implementation). Because simulation at the C++ level is fast, you can run extensive tests (random vectors, corner cases) quickly. NVIDIA’s HLS methodology, for example, leveraged this speed to run 1000× more tests at the C++ level than they could at RTL. More tests mean more confidence in your design *before* you ever generate RTL.

**Catapult C Simulation and Debug:** Catapult HLS comes with support for simulating the design within its environment as well. You can simulate the high-level code with instruments to check interface compliance or gather profiling info. If using SystemC, you’ll use the standard `sc_main` to run the simulation. For pure C++, Catapult can also integrate a testbench in the flow. Use these simulations to verify basic correctness.

**Equivalence Checking (SLEC):** One powerful feature in modern HLS flows is *formal equivalence checking* between the high-level model and the generated RTL. Siemens offers SLEC (Sequential Logic Equivalence Checker) as part of Catapult’s High-Level Verification suite. SLEC takes the C++ model and the RTL and uses formal techniques to prove that, for all possible inputs, the two representations produce the same outputs (taking into account that one is untimed/latency-insensitive and the other is cycle-specific). This is a strong guarantee of correctness that goes beyond random simulation. If SLEC passes, you can be confident the HLS did not introduce functional bugs. Running formal can catch corner-case discrepancies that tests might miss. It’s recommended to use it for critical blocks, especially if the logic is complex.

**C Property Checking:** Another relatively new feature (introduced a few years ago) is using formal methods *before* synthesis to catch issues in the C++ model itself. Catapult has a formal **C Property Checker (CPC)** that can automatically find common bugs like uninitialized variables, out-of-bounds array access, or divide-by-zero in the high-level code. These are things that might not immediately show up in simulations but could cause undefined behavior or mismatches later. By formally analyzing the C model, CPC proves properties or finds counterexamples to ensure your code is robust. You can also add your own assertions in the C code (using macros or specific syntax that Catapult recognizes) to assert things like `assert(x < 100)` if some range should hold; CPC will either prove it always holds or give a counterexample input. This is extremely helpful for ASIC design where you want to eliminate as many bugs as possible *before* committing to silicon.

**RTL Simulation & Co-Simulation:** After generating RTL, you should perform RTL simulation with testbenches. Catapult can assist by exporting a testbench that connects your original C test vectors to the RTL. In a typical co-simulation, the tool runs the C model and RTL model in tandem with the same inputs and checks that outputs match at the end of simulation (or even cycle-by-cycle if configured). Co-simulation gives confidence that the RTL behaves the same as the high-level model for those test vectors. Additionally, you might integrate the RTL into a larger system testbench (e.g., a SystemVerilog testbench or an FPGA prototype environment) to simulate it in a system context (with realistic interfaces, etc.). This is particularly important for verifying the interface logic – for instance, if you generated an AXI interface, you’d want to simulate the RTL with an AXI master driving it to ensure the handshaking works correctly.

**Coverage and Validation Metrics:** Catapult’s High-Level Verification platform also provides coverage tools at the C++ level. You can measure code coverage (which parts of your C algorithm have been exercised by tests) and even functional coverage (defining coverage points in your C model akin to SystemVerilog covergroups). High-level coverage is valuable to ensure your C tests are thorough. Since C simulation is so fast, aim for very high coverage on the C model. On the RTL side, after synthesis, you might not rerun the entire test suite due to slower simulation, but you will run a representative subset and ensure no discrepancies.

Furthermore, because ASIC projects often require verification closure with metrics like 100% code coverage and bug-free operation, using HLS doesn’t remove the need for rigorous verification – it shifts more of it to the C++ level where it’s cheaper. You still need to do lint checks, CDC checks (if multiple clocks), and formal verification on the RTL as usual (for example, ensure no CDC issues, no X-propagation, etc.). The good news is if the HLS tool is mature, the generated RTL is typically clean and one can focus on verifying functionality and integration.

**Hardware Prototyping:** While our focus is on ASIC, it’s worth noting that one verification strategy is to prototype the HLS design on FPGA to gain further confidence and perform in-circuit testing. Since Catapult can target FPGAs as well, you might generate an FPGA-optimized RTL (or simply use the same RTL if it’s generic enough) and run it on an FPGA board with real stimuli. This can flush out issues that are hard to mimic in simulation (like high-speed data issues, or uncover performance bottlenecks). However, make sure any FPGA-specific modifications (like using vendor IPs) do not drift the design away from the ASIC version functionally.

In conclusion, treat verification as a first-class citizen in HLS flow: leverage fast C simulation and formal checks at the algorithm level, then ensure equivalence and thorough testing at the RTL level. Catapult’s advanced verification features (property checking, coverage, SLEC) are especially geared to ASIC projects where **verification closure** is the longest pole. By catching bugs in the C++ stage and using formal proof of correctness, you can significantly reduce the time to reach a bug-free design ready for tapeout.

## Microarchitectural Transformations in HLS

One of the most powerful aspects of HLS is the ability to explore different microarchitectures (hardware implementations) of a given algorithm by applying transformations, either automatically or via user directives. In traditional RTL design, every change to the architecture (e.g., adding pipeline stages, duplicating hardware for parallelism, etc.) requires manual recoding. With HLS, you can often toggle a pragma or adjust a constraint to get a different architecture. Catapult HLS supports a variety of such transformations. Here we explain the key ones – **scheduling**, **binding**, **loop unrolling**, **loop pipelining**, **function inlining**, and **resource sharing** – and how they affect the generated hardware. Understanding these will allow you to guide the tool to meet performance (throughput, latency) and area goals.

### Scheduling and Binding Fundamentals

**Scheduling** is the process of assigning each operation in your C++/SystemC code to a specific clock cycle in the resulting hardware, subject to data dependencies and the target clock period. Essentially, the HLS scheduler creates a static schedule (a sequence of steps, or states) in which each operation (add, multiply, memory access, etc.) is executed in a particular cycle. If operations can happen concurrently (no dependency and enough resources), the scheduler may put them in the same cycle; if not, it orders them across cycles. The result of scheduling is often represented as a finite state machine for the control, with each state performing certain operations.

**Binding** (also known as resource binding or allocation) is the process of mapping the scheduled operations onto actual hardware functional units. For example, if your algorithm has 10 addition operations that the scheduler placed in various cycles, the binder decides whether to use a single adder unit and time-multiplex it for those operations (each in different cycles), or to use two adders and split the operations between them, etc. Binding determines the number of each type of hardware resource (adders, multipliers, memory ports, etc.) that will be instantiated, and which operations use which resource. This is closely tied to scheduling: if two operations are scheduled in the same cycle and both are additions, then binding *must* allocate two separate adders for them to run in parallel. If they are never in the same cycle, the tool has the option to use one adder for both (by reusing it in different cycles).

**Resource Sharing:** When multiple operations are bound to one functional unit, that means the unit is *shared* across those operations (at different times). This can save area because you instantiate fewer units, but might cost performance if those operations could have run in parallel but now are forced to run sequentially on the shared hardware. Catapult by default often tries to minimize resource count (for area efficiency) while meeting the throughput/latency constraints. However, you can influence this. For instance, if area is abundant and you want maximum speed, you might tell Catapult to allow more parallel hardware (no sharing). Or if area is at a premium, you might intentionally limit certain resources to force sharing. In Catapult, you can specify *resource constraints* such as “allow at most 1 multiplier” which forces all multiplies to use the same hardware (the tool will then schedule them sequentially if originally they might overlap). Alternatively, you could specify a function not to share resources internally if you want distinct hardware.

**Optimizing for Latency vs Throughput vs Area:** Scheduling can be oriented to different goals. If you want the *lowest latency* implementation (finish as fast as possible), the tool will try to schedule operations as soon as dependencies allow, potentially using more hardware to do things in parallel (more parallel = fewer cycles). If you want the smallest *area*, the tool might serialize many operations on fewer units, which increases latency but saves hardware. You can guide this via Catapult’s synthesis mode or constraints (latency-driven or area-driven). For example, Catapult’s interface might allow you to set a max latency constraint or to instruct the tool to minimize latency without regard for area, or vice versa. Ultimately, the combination of scheduling and binding decisions will determine your design’s cycle count (latency), initiation interval (throughput), and resource usage (area).

Catapult’s interactive environment even allows you to visualize the schedule and see which operations happen in which cycle, and how they map to hardware resources. This helps in understanding performance bottlenecks or resource utilization. For instance, you might see that two multiplications are scheduled far apart because only one multiplier was allocated – a hint that adding a second multiplier could let them happen concurrently and reduce latency.

In summary, **scheduling** = assigning operations to clock cycles; **binding** = assigning operations to hardware units. HLS automates these, but as a designer you should be aware of how your code and constraints influence them. Next, we discuss specific ways you can modify the schedule/binding outcome for better performance through common transformations.

### Loop Unrolling

**Loop unrolling** is a transformation that replicates the loop body multiple times to allow operations from different iterations to execute in parallel. In Catapult HLS, you can unroll loops *fully* or *partially*. A **fully unrolled** loop means the loop is completely expanded – the hardware will have as many copies of the loop body as the number of iterations, effectively eliminating the loop control overhead. A **partial unroll** with a factor `N` means the loop iterates N iterations in parallel per cycle (creating N copies of the body), and the remaining iterations if any are iterated in additional cycles.

To request loop unrolling, you can use a pragma. For example, `#pragma hls_unroll yes` before a loop will tell Catapult to fully unroll it. If you want a partial unroll, Catapult pragmas often allow `factor=<N>` or you can specify in the GUI constraint for that loop a partial unroll factor. Unrolling is only possible if the loop has a compile-time constant iteration count or an upper bound known, so the tool knows how many copies to generate. If the loop count is data-dependent, you might need to provide a maximum or manually refactor.

**Effect on Hardware:** Unrolling increases parallelism. Consider a simple loop that applies an operation on each element of an array. If unrolled by factor 2, the generated hardware will have two instances of that operation working on two array elements at a time. The loop counter will increment by 2 and the loop runs half as many cycles. Fully unrolling means if there were, say, 10 iterations, you get 10 parallel operations all done in one (or a few) cycles with no loop counter at all. The trade-off is area: each copy of the loop body consumes additional logic resources. For example, unrolling a multiply-accumulate loop will duplicate the multiplier and adder for each unroll instance. If you have plenty of silicon area and need more speed, unrolling is a good strategy. If you’re area-limited, you might avoid full unrolling.

**When to Unroll:** Unrolling is especially beneficial for loops that are not easily pipelined due to data dependencies, or for increasing throughput when loop iterations are independent. For instance, imagine a loop that shifts data in a register array (like a shift register) – to implement that in hardware, it’s often necessary to unroll it fully so that each register shift happens in parallel. In an FIR filter shift-register for input samples, the shifting of samples can be done by fully unrolling that loop so each register bit moves simultaneously to the next register. Another case is a loop that performs N independent calculations (like computing N different values); unrolling fully gives N parallel hardware units to compute all results in one go. This is how you would, for example, implement a parallel SIMD-like operation or generate multiple outputs per cycle.

**Example:** Suppose we have a loop summing 2-element pairs in an array (summing adjacent elements). Normally you would do this in one loop iterating i by 2 steps and summing `arr[i] + arr[i+1]` storing the result. To process one pair per iteration might take, say, M cycles for M/2 pairs. If we unroll that loop fully (or by factor 2), we could compute both arr\[i]+arr\[i+1] and arr\[i+2]+arr\[i+3] at the same time, effectively halving the number of cycles. The hardware would have two adders operating in parallel. The loop index logic is adjusted accordingly. In code it might look like:

```cpp
int sum_pairs[50];
#pragma hls_unroll factor=2
for(int i=0; i<100; i+=2) {
    int j = i/2;
    sum_pairs[j] = arr[i] + arr[i+1];
}
```

With factor=2, the loop body is replicated twice, so in one iteration of the loop, two sums happen. The resulting hardware has 2 adders. If fully unrolled (factor = 50 in this case, since 100 elements /2), you’d get 50 adders and all sums done in essentially 1 cycle.

**Constraints:** Only unroll when there are no loop-carried dependencies or if you are intentionally duplicating logic to handle them. If a loop iteration depends on the previous one’s result, fully unrolling might logically still work but yield a big combinational loop (which likely fails timing). HLS might refuse to unroll a loop with an unresolved dependency unless you explicitly tell it and know what you’re doing. Often, pipelining is better for dependent loops, while unrolling is great for independent computations.

In Catapult’s UI, you can also set a loop to unroll via a checkbox or constraint. For example, one might *select the loop in the tool’s interface and check “Unroll”* to fully unroll it. This will appear in the generated schedule as the loop gone and operations parallelized.

In summary, loop unrolling is your go-to for extracting parallelism from iterative computations. Use it to trade area for speed, and target it at loops where iterations are independent (or where partial unroll can balance the trade-off). It’s a straightforward way to exploit the inherent parallelism in algorithms via HLS.

### Loop Pipelining

Loop pipelining is arguably the most powerful optimization in HLS for improving throughput. When you pipeline a loop, you allow a new iteration of the loop to start before the previous one finishes, by overlapping their execution in a hardware pipeline. This yields a significant increase in throughput, measured by a smaller initiation interval (II). A perfectly pipelined loop can reach an II of 1, meaning one new iteration (and potentially one new result) every clock cycle.

To pipeline a loop in Catapult, you use a pipeline directive. In code, it may look like `#pragma hls_pipeline_init_interval 1` before the loop, which attempts to achieve an II of 1 (the `init_interval` pragma). There might also be a pragma to set the pipeline “flush” or “stall” mode (Catapult allows different pipeline behavior when the loop ends; `pipeline_stall_mode flush` is one option as seen in a SystemC example, which means the pipeline will flush out remaining operations at loop exit). In the GUI, you could set a loop’s initiation interval constraint.

**Effect on Hardware:** When a loop is pipelined, the HLS tool will create a multi-stage pipeline for the loop body. Each stage corresponds to a portion of the loop’s operations. Once the first iteration goes into stage 1 and moves to stage 2, stage 1 is free to take the next iteration, and so on. After the pipeline is filled, you get one iteration completed per cycle (if II=1). The latency of one iteration might be multiple cycles (equal to the pipeline depth), but the throughput is high.

A simple analogy: if a loop takes 100 cycles without pipelining (executing one iteration after the other), with pipelining it might take, say, 10 cycles to fill the pipeline and then 1 new result each cycle, so 100 iterations might take \~109 cycles (fill + 99 subsequent cycles) instead of 100\* some factor without pipeline. In large loops, the difference is huge for throughput. For example, an unpipelined loop of 192 steps would produce results much slower than a pipelined version producing one per cycle; indeed, not pipelining can lead to *huge* latency and low throughput, whereas pipelining “fixes” this by overlapping operations. In an FIR filter case study, a loop that took 192 cycles unpipelined (doing all operations sequentially) was improved such that with pipelining the effective throughput was one result every cycle, dramatically reducing the total runtime.

**Example:** Consider a loop that adds each element of an array to an accumulator (summing the array). This loop has a dependency: the accumulator value depends on the previous iteration. If you pipeline this naive loop, the dependency will actually prevent an II of 1 (because you can’t start adding the next element until the previous sum is done). The tool might achieve II=1 but it will internally create feedback registers to handle it or end up II=2 if the feedback is a bottleneck. Now consider a loop where each iteration is independent (say, writing an output array from an input array with some function). That can be pipelined with II=1 easily. A more complex example: a loop performing a MAC (multiply-accumulate) for a dot product has a dependency on the accumulation, but by pipelining, each multiply can be made a separate stage so that while one iteration is adding to partial sum, the next iteration can start a multiply for the next data – in effect, it’s doing something akin to a sequential accumulator but in a pipelined manner (though ultimately the dependency likely imposes II=1 with a certain pipeline depth to accommodate the adder latency).

**Pipeline Schedule Visualization:** In Catapult’s analysis, a pipelined loop is often depicted with a feedback initiation interval and pipeline stages. You might see an arrow in the schedule indicating that the loop is pipelined. If you hover over pipeline stages, you see shading indicating the stage boundaries. The area report might show a slight increase in registers for pipeline registers, but often the combinational area stays similar. Indeed, pipelining typically has **minimal area impact** if the loop is not unrolled – it mostly adds pipeline registers and perhaps some control logic. The FIR filter example showed only a marginal area increase when they pipelined the loop, yet performance improved drastically. That’s the magic of pipelining: you get a big win in throughput with a small cost in registers.

**Pipelining vs Unrolling:** It’s important to differentiate these. Unrolling creates multiple independent operations in parallel *in the same cycle*. Pipelining keeps a single operation unit but overlaps its usage across cycles. For instance, if you have one multiplier and you pipeline a loop that uses it every iteration, that single multiplier will be in use every cycle after the pipeline fills – effectively achieving throughput 1 result/cycle with one multiplier (assuming no other bottlenecks). If you unrolled the loop fully with, say, 64 multipliers, you could potentially get the result in 1 cycle latency, but you had to use 64 multipliers to also get 64 results in 1 cycle (which is overkill if results are needed sequentially). The pipelined single multiplier approach gives you one result per cycle too (after some initial latency) using a fraction of the area (only one multiplier). This was observed in the FIR case: by pipelining, they achieved high performance while still only using a single multiplier hardware (Catapult intelligently reused the multiplier each cycle). Resource reuse and pipelining often go hand-in-hand: you design a deep pipeline so that one functional unit can sustain a throughput of 1 per cycle.

**When to Pipeline:** Almost always, pipeline innermost loops that handle streams of data to maximize throughput, unless the loop is very small or combinational and already meets timing easily. If loops have dependencies that make II > 1, sometimes you can restructure the algorithm to break the dependency or use a different approach (for example, unrolling a reduction tree instead of sequential accumulate, or using dual accumulators to alternate). Also, pipeline outer loops if the inner loops are small or unrolled. Catapult can pipeline both inner and outer loops, but usually you pipeline the most compute-intensive loop. If you have nested loops, you might pipeline the inner loop to accelerate its operation, and possibly the outer loop as well if it’s feeding new data sets (this requires more consideration of data availability and buffering).

**Stall vs Flush:** A quick note on pipeline stall vs flush modes (Catapult’s directive): In *stall* mode, if new data stops coming, the pipeline will stall holding its state (waiting until more iterations or some condition). In *flush* mode, if the loop ends or data stops, the pipeline will continue to flush out any in-flight operations and then gracefully empty. Flush is simpler for one-off loops that run a fixed number of iterations then end; stall can be useful in while(1) loops or streaming processes where you want the pipeline to pause when no data but not clear out intermediate state. Typically for a loop that runs a known count, flush is fine and ensures the pipeline empties at loop completion.

In summary, **loop pipelining is critical for achieving high throughput in ASIC HLS designs**. It allows you to get maximum utilization of functional units and meet high data rates without replicating hardware unnecessarily. Always consider pipelining for performance-critical loops – it often gives you the best of both worlds: speed and efficient resource usage.

### Function Inlining and Hierarchical Optimization

In HLS, a *function call* can either be preserved as a separate module (component) or inlined into the caller. **Function inlining** means the HLS tool will treat the function’s code as if it were written inline in the caller, allowing optimizations across what was originally a function boundary. In Catapult, you can control this with pragmas/attributes (for example, `#pragma inline` on a function, or on the call site). By default, Catapult might inline small functions automatically, but larger ones or those meant to be reused might remain separate.

**Why Inlining Matters:** If a function is not inlined, Catapult will generate a distinct hardware block for it and the caller will instantiate that block (much like how a module instantiation in RTL works). This can be useful for reusability – e.g., if you call the same function in multiple places or multiple times, one instance of hardware can service all calls (one after the other, if scheduling allows). However, not inlining also means the interface between the caller and callee becomes a fixed boundary, which might introduce extra latency (the call/return or handshake overhead) and limit optimizations (the tool cannot move operations across the boundary easily).

Inlining removes that boundary: the operations inside the function become part of the caller’s control flow. This often enables better scheduling because the tool sees the bigger picture. For instance, if the caller has an operation that can run in parallel with something inside the callee, that’s only possible if inlined (otherwise, it would treat the whole function call as one schedule entity). Inlining can also expose more opportunities for resource sharing or common subexpression elimination across what used to be different functions.

**Trade-offs:** The downside of inlining is potential duplication of hardware. If you call a function twice in parallel (or nearly parallel), inlining will create two separate hardware implementations (one per call site) if the scheduler decides they need to run concurrently. If instead you kept it separate, the same hardware could have been invoked twice (one after the other). So, you need to choose: if the function is performance-critical and you might benefit from parallel execution, inlining is good (you allow two instances to form if needed). If the function is large and you only ever need to execute it once at a time (even if called from multiple places sequentially), keeping it as a separate component can save area by reusing it.

For example, suppose you have a compression function you call on two different data sets sequentially. If inlined, you get two copies of that logic (if the calls are in separate parts of code not overlapping, the scheduler might still just reuse one hardware though, depending on control flow – HLS can sometimes reuse even inlined logic if it knows the calls are disjoint in time). If not inlined, you definitely have one hardware and the calls share it (with some multiplexing of inputs).

**Inlining in SystemC vs C++:** In SystemC, one might have multiple `SC_MODULE`s in a hierarchy. Catapult can either flatten the hierarchy or keep some modules separate (perhaps if you intend to isolate a block for separate synthesis). Flattening (analogous to inlining) the modules can allow cross-boundary optimization but you might lose some modularity. Typically, HLS flows flatten everything unless told not to, because cross-hierarchy optimizations yield better results. But if you had, say, an IP block you don’t want changed, you’d keep it separate.

**When to Inline:** It is often beneficial to inline small utility functions (like a math helper) to reduce function call overhead. It’s also useful to inline a function that is called in a loop for each iteration, so the loop and function can be optimized together. Conversely, if you have a large function that you call only once, it doesn’t matter either way – but if it’s only once, inlining won’t increase hardware (just merges it into caller). If you have a function called from multiple independent places that will never run concurrently (like an initialization function and a processing function called one after another), you might not need to inline because the tool can reuse the hardware for both calls even without inlining (Catapult can schedule one after the other on the same component). However, if those calls *could* overlap or if you want to pipeline through the function boundary, then inlining (or using HLS dataflow across functions) is needed.

**Example:** Imagine an AES encryption function that processes a 128-bit block of data. You might write an AES round function and call it 10 times in a loop (for 10 rounds). Unrolling the loop vs inlining the function are two considerations here. Possibly you inline the round function to allow unrolling the loop fully and thus implement all 10 rounds as separate hardware stages (which would be like a pipeline or unrolled loop). If you didn’t inline, the tool might treat it as the same hardware round function called 10 times sequentially, which could lead to using one round function iteratively (smaller area but 10 cycles latency). If you want a fully unrolled pipeline of 10 rounds (to get one-round-per-cycle throughput), you’d inline and unroll. If you want a smaller area iterative solution, you might leave it and let the loop iterate using one function hardware.

In summary, **function inlining is about removing hierarchy boundaries to enable other optimizations.** Use it when hierarchy is getting in the way of performance. Be mindful that it can increase hardware if the function is invoked multiple times concurrently, but that may be exactly what you want for performance. For a clean HLS design, you might start with everything inlined (or one flat function) to get the best performance, then decide if any portion could be separated to save area and still meet requirements.

### Resource Sharing and Duplication

We touched on resource sharing in the scheduling/binding section, but let’s delve a bit more. **Resource sharing** is when multiple operations use the same hardware resource at different times. **Resource duplication** (or not sharing) is the opposite: giving each operation or each concurrent operation its own hardware unit. HLS tools by default try to minimize resource count to save area, but they will not share resources if it would violate a timing constraint or throughput requirement.

**Examples of Resource Sharing:**

* If you have two multiplications in your algorithm that *never execute in the same cycle* (perhaps one is in a part of the code disjoint from the other, or sequentially in a loop), the tool can map them to one single multiplier hardware. That multiplier will be time-multiplexed: it computes one result, then the other. The benefit is you only pay silicon for one multiplier instead of two. The cost is potentially increased total latency if those operations were on the critical path.
* If those two multiplications could have been done in parallel to shorten latency, forcing them on one multiplier means you lost that opportunity. HLS will only do that if you either told it to optimize for area or if doing them in parallel wasn’t beneficial due to some other constraint.

**How to Control Sharing:** Catapult likely has directives or constraints for this. One method is specifying *max instances* for a given operation type. For example, “use at most 1 multiplier” or “allow 2 adders”. In some HLS tools this is a pragma like `#pragma HLS RESOURCE core=Mul limit=1` or similar. In Catapult’s GUI, you might go to *Architectural Constraints* and set a specific number of units for an operation. The FIR filter lab instructions show steps like *selecting a loop or operation and unrolling or adjusting resources*. If pipelining is enabled and you didn’t unroll, Catapult might naturally reuse hardware across loop iterations since each iteration uses the resource in a different cycle (that’s an automatic form of sharing in time).

**Binding in Catapult typically does sharing by default** unless operations are in the same cycle. If you want more sharing than default, it usually means telling the tool to serialize some things that it would normally do in parallel. If you want less sharing (more parallel hardware), you might unroll or explicitly allow multiple units. For instance, if a loop is pipelined with II=1 using one multiplier, you get one multiplier that’s busy every cycle (good throughput, minimal area). But maybe your latency from start to finish is too high because the pipeline depth is long; you could unroll parts of the loop to use more multipliers in parallel, which shortens latency at the cost of multiple multipliers. Another scenario: if you have an algorithm that does two different multiplications at roughly the same time in the schedule, the tool by default will allocate 2 multipliers (because it has to). If you absolutely had to reduce area, you could relax a throughput requirement or introduce a scheduling constraint that these two operations should not happen in the same cycle (thus allowing a single multiplier to handle them one after the other). That is rarely done unless you have specific knowledge and flexibility in throughput.

**Resource Sharing and ASICs:** In ASIC design, area costs are important, but if the chip can afford a bit more area to meet timing, designers often err on the side of adding some hardware to simplify timing closure. However, with HLS, since the initial code might inadvertently cause too much parallel hardware (especially if you unroll a lot), you might then use sharing to dial back the area. It’s a knob to balance area vs performance. In FPGAs, resource sharing is sometimes less beneficial because unused LUTs don’t cost anything extra (they’re just available), and using more smaller adders might map better to FPGA primitives. But in ASIC, every gate costs power and area, so sharing can directly translate to lower power and smaller die if the performance is sufficient.

**An example of too much sharing** might be if the tool tries to use one memory port for two array accesses by scheduling them sequentially, but this creates a long loop latency. If latency is critical, you’d prefer to have two memory ports (which might mean duplicating or partitioning the memory) to allow parallel access. That is reducing sharing on memory. So resource sharing is not only about ALUs but also about memories and busses.

**Concurrent vs Exclusive Operations:** HLS analyzes your code’s control flow to know what can run concurrently. If two operations are in the same basic block without dependency, they can run same cycle (thus need separate resources). If they are in mutually exclusive branches of an if-else, they will never execute at the same time, so the tool could map them to the same hardware with a multiplexer on the inputs, since either one or the other will be needed. This is a form of *control-flow based sharing*: e.g., one multiplier used for two different computations that occur in different branches of an if. The generated hardware would have a mux and some gating to route the correct operands to the multiplier depending on the branch taken. This is a classic optimization HLS does to save area when you have mutually exclusive operations.

**By understanding this, you can write code to maximize such sharing if needed** (or avoid too many concurrent operations if you’re trying to keep area low). Alternatively, you write everything as if it could be parallel (for clarity) and then rely on directives to limit resources after the fact.

### Summary of Transformations

To wrap up this section, here are the key transformations and their purpose in brief:

* **Loop Unrolling:** Replicate loop iterations to increase parallelism (throughput, lower latency) at the cost of more hardware. Good for independent iterations or when needing multiple operations at once.
* **Loop Pipelining:** Overlap consecutive loop iterations in a pipeline to dramatically improve throughput (one result per cycle possible) with minimal extra hardware. Critical for high-throughput designs.
* **Function Inlining:** Flatten function hierarchy to allow cross-boundary optimization and potentially parallel execution of what were separate function calls. Improves scheduling flexibility, may duplicate hardware if needed for performance.
* **Resource Sharing:** The flip side of duplication – use one hardware resource for multiple purposes to save area. The tool handles a lot of this by default, but you can adjust the degree of sharing with constraints. Useful for area optimization when peak parallel performance isn’t required.
* **Scheduling Control:** Using constraints to influence the schedule (e.g., limit the latency of a loop or the II). For instance, you might specify a loop must complete in at most X cycles, and the tool will try to meet that by maybe unrolling or using more resources.
* **Binding Control:** Guiding how operations bind to resources (e.g., fix the number of certain units). This ties directly into sharing and parallelism decisions.

Each of these can be applied selectively. The real power of Catapult HLS is in trying different combinations quickly to see their effect. It’s not uncommon to do several syntheses: one with fully unrolled loops to see max performance, one with fully pipelined but not unrolled loops to see the area/performance trade, etc. Because this is much faster than writing RTL by hand each time, you can explore the design space and converge on an optimal microarchitecture for your ASIC. HLS reports will show you how these transformations affect metrics like latency (number of cycles), interval (throughput), and resource count (number of adders, multipliers, registers, memory bits, etc.), helping you make informed trade-off decisions.

## New Features in Latest Catapult HLS (ASIC-Focused)

The Catapult HLS platform has evolved in recent years, adding several features that enhance its capabilities for ASIC design. Here we highlight some of the **latest features (as of 2024-2025)** that are particularly relevant to ASIC flows:

* **Physically-Aware Synthesis:** Modern Catapult can perform *physically-aware optimizations*, taking into account backend effects like placement, wire delays, and multi-Vt (multi-threshold voltage) optimization. In practice, this means Catapult doesn’t operate in isolation from layout concerns – it can optimize the generated RTL for better timing/area by considering physical information. For example, it might decide to pipeline a long combinational path not just because of logic delay, but also anticipating long wire delay in an ASIC. Multi-Vt support allows the tool to predict power vs speed trade-offs, perhaps using fast cells on critical paths and slow, low-leakage cells elsewhere (this is often done in backend, but early estimates can guide microarchitectural choices). These features help ensure the HLS output can close timing in silicon with fewer iterations.

* **Integrated Low-Power Optimizations:** Catapult has integrated the Siemens PowerPro technology into the HLS flow. PowerPro was traditionally a tool to add clock gating and memory gating to RTL designs to reduce dynamic power. Now in HLS, these optimizations can be applied at the C++ level or during RTL generation. The result is that Catapult can automatically insert clock gates for registers or blocks that are idle, optimize gating on memories when not reading/writing, and generally produce an RTL that is power-optimized out-of-the-box for ASIC. Early power estimation is also provided, so you can see how changes in your C++ code or HLS directives affect power consumption. This is especially important in ASIC where power budgets are tight – having these estimates and optimizations at a high level means you can iterate on power (e.g., reducing switching activity by altering an algorithm or adding gating conditions in C++) before ever running a full gate-level power analysis.

* **Catapult for AI/ML (Catapult AI and hls4ml):** A very recent addition is **Catapult AI** and specifically the **Catapult AI NN** solution, which streamlines the implementation of neural network models on ASICs. This feature integrates the open-source *hls4ml* framework (originally developed for FPGA firmware by CERN and others) with Catapult, enabling designers to take a neural network described in a high-level framework (like TensorFlow or PyTorch) and automatically generate a synthesizable C++ model which Catapult then turns into optimized RTL for ASIC. Catapult AI NN comes with a library of parameterized C++ components (layers, activation functions, etc.) tailored for ASIC implementation. These allow exploration of quantization (e.g., different fixed-point precisions for neural net weights/activations) and resource trade-offs (like reuse factors, loop unrolling in the neural network inference loops) at a high level. This feature is significant given the rise of AI accelerators – it helps bridge the gap for software-oriented AI developers to create power-efficient ASIC accelerators without deep hardware expertise. For example, designers can adjust latency and resource trade-offs of a neural network by modifying C++ parameters and immediately see PPA impact, choosing the best architecture for their ASIC. Catapult AI NN essentially extends HLS to a domain-specific level for machine learning, and its availability (as of Q4 2024 general release) shows Siemens’ focus on ASIC accelerator design flows.

* **High-Level Verification (HLV) Suite:** As discussed earlier, Catapult now includes a comprehensive verification toolkit for HLS. Newer versions emphasize **HLS-aware verification**, such as *Catapult Coverage* for measuring coverage on HLS designs and *Catapult Formal* for property checking. The C Property Checker (CPC) is one piece of this (catching C++ bugs with formal methods). Another piece is integration with SystemVerilog assertions and coverage – for instance, Catapult can take assertions written in the C++ model (or added via pragmas) and turn them into SystemVerilog Assertions (SVA) in the generated RTL. This means your high-level assertions carry through to implementation, aiding in verification closure. The tool can also insert *coverage points* from C++ into the RTL. The ultimate goal is often described as **C++/SystemC sign-off** – verifying at the high level so thoroughly (with 100% coverage and formal proofs) that you can sign off the design at that level, trusting Catapult to generate correct RTL. While in practice one would still verify RTL, the gap is closing. The new formal equivalence checking (SLEC) improvements and coverage tools help ensure that by the time your design is in RTL, it’s a formality to verify, as most issues were caught in the HLS stage.

* **Enhanced Language and Library Support:** Recent Catapult versions have kept up with modern C++ standards and added support for useful synthesis constructs. For example, support for C++14/17 features where relevant (some high-level constructs can be synthesized as long as they boil down to static structures or loops). The inclusion of NVIDIA’s **MatchLib** library is also a key feature. MatchLib provides synthesizable transaction-level components like network-on-chip routers, load/store units, and FIFO frameworks that greatly ease building complex SoC components in SystemC. It was originally open-sourced by NVIDIA to aid HLS adoption and now being part of Catapult’s offering means users can model more complex behaviors (like out-of-order memory accesses, or streaming networks) with pre-built components. For ASIC designers, this means less time reinventing on-chip communication or glue logic – you can reuse these high-level components and trust that Catapult will synthesize them efficiently.

* **ASIC/FPGA Independence and eFPGA support:** Catapult continues to ensure that the C++/SystemC input is agnostic to the implementation fabric. You can use the same high-level code to target ASIC libraries or FPGAs or even embedded FPGAs (eFPGA). Under the hood, the tool will use appropriate optimization strategies for each (for instance, understanding FPGA primitives vs. ASIC cell libraries). Recent improvements likely include better inference of FPGA DSP blocks or ASIC multi-bit adders, etc., but since we focus on ASIC, the key point is you don’t have to write your code differently – just retarget the tool. This is useful if you prototype on FPGA and then move to ASIC; Catapult can retarget the design to an ASIC library (e.g., TSMC 7nm or GlobalFoundries 22nm) and generate new RTL optimized for that. One Reddit user mentioned simply targeting a GF22nm library with Catapult and getting ASIC-ready HDL out – highlighting that Catapult handles the low-level differences in timing and coding style for you.

* **Improved GUI and Analysis Tools:** The Catapult Design Analyzer GUI has been refined to better visualize the results. Designers can graphically see the pipeline structure, state machine, and even generate schematics of the synthesized hardware at a high level. It can highlight critical paths and resource bottlenecks, guiding you where to apply directives. New versions might also integrate power analysis views (so you can see estimated toggle rates per operation) and more “what-if” analysis features.

To sum up, the latest Catapult HLS versions bring a focus on ASIC-centric needs: **power optimization, physical awareness, AI accelerator support, and robust verification**. These features make HLS more practical and reliable for complex ASIC projects, reducing the risk and effort traditionally associated with going from C++ to silicon. They also widen the scope of HLS to more application domains (like AI) and ensure that even as designs push the envelope (higher frequencies, lower power), the HLS tool can keep up with advanced optimization techniques.

## Practical Design Case Studies

To solidify these concepts, let’s walk through a couple of design scenarios using Catapult HLS targeting ASICs. These case studies will illustrate how we apply the HLS methodology and transformations in practice, along with results and trade-offs.

### Case Study 1: FIR Filter Accelerator (DSP Example)

**Problem:** Design a Finite Impulse Response (FIR) filter hardware block that takes a stream of input samples and produces filtered outputs. Suppose it’s a 64-tap FIR (i.e., 64 coefficients). The goal is to achieve high throughput (one output per cycle) with minimal hardware resources, targeting an ASIC implementation at, say, 200 MHz.

**High-Level Model:** We write a C++ function for the FIR algorithm:

```cpp
#define N 64
void fir_filter(const int input_sample,      // single input (streaming)
                int &output_sample) {        // single output
    static int shift_reg[N];      // shift register to store last N inputs
    static int coeff[N] = { /* 64 filter coefficients */ };

    // Shift and insert new sample
    for(int i = N-1; i > 0; --i) {
        #pragma hls_unroll yes
        shift_reg[i] = shift_reg[i-1];
    }
    shift_reg[0] = input_sample;

    // MAC: multiply-accumulate over the 64 taps
    long acc = 0;
    for(int j = 0; j < N; ++j) {
        acc += (long)coeff[j] * shift_reg[j];
    }
    output_sample = (int) acc;
}
```

In this code, we maintain a static shift register for the last 64 inputs (this will infer registers or memory in hardware). We then compute the dot-product of the coefficient and register arrays for each new input. As written, the code processes one sample at a time (one function call per sample). We can use HLS directives to optimize this.

**Optimizations:**

* We fully **unrolled** the loop that shifts the register (`#pragma hls_unroll yes` on the shift loop) to implement the 64-stage shift register with parallel moves. This is necessary because synthesizing a loop that iterates 64 times to shift data could either infer a chain of registers (with each register’s next input from the previous one’s old value) or use one register and a loop, but by unrolling we explicitly create 64 registers connected as a shift chain. The tool will recognize the unrolled loop as 64 parallel assignments – effectively a 64-bit wide shift register implemented with flip-flops. Fully unrolling here ensures the shift happens in a single cycle. If we didn’t unroll, the loop might take 64 cycles per input just to move data, which is unnecessary in hardware.

* The MAC (multiply-accumulate) loop is a reduction loop. It has a dependency (the `acc` accumulative sum), which makes it less straightforward to parallelize. By default, Catapult will likely schedule this loop to take 64 cycles (one multiply-add per cycle) if left unoptimized – that would be too slow (throughput one output every 64 cycles). To meet our goal of one output per cycle, we need to pipeline this loop heavily. We can apply **loop pipelining** on the MAC loop. For example, we might add `#pragma hls_pipeline_init_interval 1` before the inner loop. This tells Catapult to initiate a new iteration of the MAC loop every cycle. The dependency on `acc` means the adder cannot produce the final sum in one cycle, but Catapult will pipeline the additions across cycles. Essentially, it builds a pipelined adder tree or accumulative pipeline so that while one sum is partial, the next input’s multiply can start accumulating in another stage. The end result can be a design that produces a completed output every cycle with a pipeline depth equal to 64 (the latency is 64 cycles, which is fine). We also might instruct Catapult to **pipeline the outer function** (so that it can accept a new input each cycle, continuously). In Catapult GUI, this might involve setting the function (or outer loop around the function, if it were in one) to II=1.

* Another optimization: **Resource sharing vs duplication**. With the MAC loop pipelined and II=1, Catapult can implement this with a single multiplier and a single adder that are reused every cycle for different partial sums (like a rolling accumulation). This is what happened in a similar FIR design: after pipelining, the tool still only used one multiplier – it just kept it busy every cycle with new data. That yields the desired throughput with minimal resources, albeit a latency of 64 cycles. If we wanted to reduce the latency (say we needed the output with less pipeline delay), we could unroll the MAC loop partially or fully to use more multipliers in parallel. For instance, unrolling by factor 4 would use 4 multipliers working on 4 different taps at once, cutting the accumulation depth by 4 (latency \~16 cycles, and perhaps II can still be 1). Fully unrolling the MAC loop (factor 64) would instantiate 64 multipliers and 64 adders in a tree, producing the output in 1 cycle latency (combinatorially) – but that’s a huge area increase and likely not needed for throughput because we can already get one per cycle with one multiplier. So we chose to pipeline rather than unroll the MAC for efficiency.

**Results:** By applying loop unrolling to the shift register and loop pipelining to the MAC, we achieve one output every clock cycle (after an initial fill latency of 64 cycles). The area usage is moderate: 64 registers for the shift register, 1 multiplier, 1 adder (plus some registers for pipelining the accumulator). The schedule in Catapult would show the MAC loop pipelined with II=1 – indicated by pipeline stage shading and an initiation arrow. The throughput is maximized. Catapult’s report would show latency \~64 cycles per sample (which is the pipeline depth, essentially the filter length) and II=1 (one new sample each cycle).

Without pipelining, as a comparison, the design would have taken \~192 cycles per sample (64 for shift + 64 for MAC, plus overhead) – clearly too slow. Pipelining fixed this performance issue and only marginally increased area (the registers for pipeline). The unrolled shift loop did increase area (we created 64 registers, but that’s necessary hardware for the function anyway).

We also check the tool’s area report: minimal logic besides the multiplier and adder. If area was a concern and 200 MHz timing was easy, we could even explore using a slower clock and time-multiplexing the multiplier more (but here we already are multiplexing it fully across cycles). Alternatively, if performance needed to be even higher (say 2 samples per cycle), we could unroll the outer function loop – effectively instantiating two filters in parallel, but that doubles area.

**ASIC Considerations:** We would ensure that the multiplier is pipelined enough to meet 200 MHz (Catapult would schedule the multiply over one cycle if the library multiplier can finish in <5ns, or it might use two cycles if not). Also, we’d check that the routing of 64-bit adder tree doesn’t become a timing issue – physically-aware HLS might prompt adding pipeline stages in the adder tree if needed. But given we chose sequential accumulation, we avoided a large adder tree.

This FIR example demonstrates the power of loop pipelining in achieving high throughput with minimal hardware. It also shows how HLS lets us easily try different architectures (one multiplier reused vs multiple multipliers) by adjusting pragmas. The end design is tailored for ASIC: it uses a lot of sequential depth but saves area, which is often fine in ASIC where a few hundred flip-flops are negligible but 64 multipliers would be very large. Indeed, pipelining was very effective, yielding a high-performance design with little area penalty. This matches the expectation that a 64-tap FIR *can* be done with one multiplier and still output a result each cycle – HLS achieved that, whereas a naive RTL designer might have first thought 64 multipliers were needed for one-per-cycle throughput.

### Case Study 2: Neural Network Softmax Accelerator (Algorithmic C Library Example)

**Problem:** Implement a hardware accelerator for the **softmax** function, commonly used in neural networks. Softmax takes a vector of values (logits) and computes exponentials and a normalization so that the outputs are probabilities summing to 1. This involves non-linear functions (exp) and division, which are expensive in hardware if done in floating-point. We want to target an ASIC implementation with fixed-point arithmetic for efficiency. The design should be easily configurable for different precisions and vector lengths, highlighting HLS’s strengths in reuse and parameterization.

**High-Level Model:** We leverage an existing implementation from Mentor’s algorithmic C (AC) library: `ac_softmax_pwl`, which computes softmax using a piecewise-linear (PWL) approximation for the exponential. This function is already optimized for fixed-point input and output. We decide to integrate this into our accelerator. Our top-level will take an array of input fixed-point values and produce an array of output fixed-point values (softmax probabilities). For example:

```cpp
#include <ac_math/ac_softmax_pwl.h>  // Hypothetical include for the softmax PWL function
using input_t = ac_fixed<16,8,true>;   // 16-bit signed fixed, 8 integer bits
using output_t = ac_fixed<16,8,false>; // 16-bit unsigned fixed, 8 integer bits (for probabilities)

#pragma hls_design top
void softmax_accel(const input_t in[NUM], output_t out[NUM]) {
    ac_softmax_pwl<input_t, output_t, NUM>(in, out);
}
```

Here, `NUM` is the number of logits (say 20 or could be 100, etc., configurable). We defined fixed-point types with 16 total bits and 8 integer bits, which gives us a certain precision. The AC library’s `ac_softmax_pwl` will internally use a lookup or piecewise linear segments to approximate exp() and perform division by the sum of exponentials, all in fixed-point. The nice thing is this function is already tested and optimized. We simply call it, and by marking our function as `hls_design top`, Catapult knows this is the top-level to synthesize.

**Optimizations and Features:**

* This design relies on a pre-optimized library function, so we as designers did not manually unroll or pipeline inner calculations – we trust the library (which itself likely contains HLS pragmas or is written in a pipeline-friendly way). The library might internally pipeline the computation so that it can output one softmax result per cycle (with a certain latency). If not, we could apply directives at the function call level. For example, if `ac_softmax_pwl` processes the vector in a loop, we could attempt to pipeline that loop. In Catapult, we might ensure that the softmax function itself is inlined so the tool can see inside it and optimize across our top-level if needed.

* We choose fixed-point precision appropriate for our needs. If we find the accuracy isn’t sufficient, we can increase the bit-width (say 20 bits or adding more fractional bits) and re-synthesize. This will increase area slightly but improve accuracy. Conversely, if we’re over-designing, we could reduce precision. This parameter tweak and re-synthesis cycle is very quick in HLS, demonstrating agility in design-space exploration. The AC datatypes make it easy: just change template parameters and recompile HLS – no manual re-implementation of arithmetic.

* **Interface considerations:** The function as written has array inputs and outputs of length `NUM`. In hardware, Catapult will likely implement this as memory burst reads and writes or as a streaming interface. Since `NUM` could be, for instance, 20 (as in an example), which is small, the tool might just create 20 parallel ports or a simple loop to read 20 values one by one from an interface. Usually, though, an array argument becomes an interface that reads a stream or memory. We might instruct Catapult to use a handshake protocol for the array: e.g., an AXI-Stream for input and output. Alternatively, if this accelerator is part of a larger SoC, we might integrate it such that it reads from a shared memory. In the ESP framework from Columbia, for instance, they treat input and output arrays by allocating a scratchpad and the accelerator loads them via DMA. The tutorial mentions focusing on interface and data movement for softmax, highlighting that moving data in/out was more the challenge while the compute was handled by the library.

* **Performance:** Softmax involves exponentials and divisions, which are more complex than simple add/multiply. By using piecewise linear approximation and fixed point, we’ve made it hardware-friendly. Suppose it takes, for argument’s sake, 5 cycles to compute a softmax of 20 values (maybe pipelined). That’s fine for many applications. If we needed higher throughput (like processing many vectors of logits), we could pipeline feeding multiple input vectors. For example, after one vector is in process, start reading the next – this would require a dataflow approach with ping-pong buffers or a streaming design. Catapult AI NN, as mentioned, would help integrate this if we had a full neural network, but here it’s just one layer.

**Results:** Using HLS, we integrate a complex function in a few lines of code and get an accelerator. The output probabilities have accuracy determined by our chosen precision, and we can tweak that. The design is highly reusable – if tomorrow we need a softmax for 256-length vector, we change `NUM` and perhaps adjust bit-width (maybe more integer bits if summing many values). Catapult will resynthesize and give us new RTL. Try doing that in hand-written RTL – it’s tedious and error-prone, whereas HLS handles the scaling easily.

One important outcome is that this design achieves the functionality with a high-level library, demonstrating **productivity**. The Mentor/Siemens AC library provides many such functions (CORDICs, filters, linear algebra, etc.), meaning HLS users often don’t need to design from scratch – they can assemble these components. In an ASIC project, this could save months of work and also leverages well-verified implementations.

**ASIC Considerations:** We would simulate the C++ model with many random vectors to ensure the precision is adequate (maybe compare against a double-precision reference to ensure error is within acceptable bounds). Because we used formal PWL approximations, we’d carefully check edge cases (like very large or very small inputs, to see if overflow or underflow occurs). We might use Catapult’s formal tools to prove no overflow happens for expected input range, for example. After synthesis, we’d use SLEC to ensure the RTL matches the C++ model – especially important here since approximations are involved; we want to be sure the piecewise segments were implemented correctly.

Power-wise, this accelerator will largely be dominated by the exponent calculator and multiplier for the division normalization. We might examine the toggle rates – maybe we find we don’t need to run it at full clock speed all the time, so adding an enable to only compute when new data is ready could be worthwhile. Catapult might automatically clock-gate if it sees the function is not always active.

In summary, the softmax case study shows how HLS and libraries empower us to tackle complex math on ASIC with relative ease and flexibility. The emphasis was on using fixed-point for ASIC efficiency, and it highlights the ability to adjust bit-width and use specialized libraries (which is a new-ish feature that such libraries are available and integrated). The design focuses on data movement as much as computation, which is typical in ASIC accelerators: moving data to/from memory efficiently is often the challenge, and compute might be a single function call. By focusing on the interface (e.g., ensuring a contiguous burst of 20 is read, processed, written back) and using HLS to handle the compute, we get a balanced solution.

---

These case studies illustrate a beginner-friendly example (the FIR, which primarily needed loop pipeline/unroll decisions) and a more advanced one (softmax, leveraging libraries and focusing on fixed-point and integration). In both cases, Catapult HLS allowed exploring different microarchitectures easily and produced RTL suitable for ASIC implementation.

## Conclusion

High-Level Synthesis with Catapult opens up a new world of design possibilities for ASIC developers of all experience levels. Beginners can start from a clear algorithm in C++ or SystemC and get working hardware without hand-coding Verilog, while experts can guide the tool to refine the microarchitecture for optimal PPA. In this tutorial, we covered how to model your design behaviorally, specify interfaces and clock constraints, and apply HLS transformations like pipelining, unrolling, inlining, and resource sharing to meet design goals. We also discussed advanced features of Catapult – from power optimizations to formal verification and specialized flows for AI accelerators – which address the needs of modern ASIC designs.

By following a systematic HLS design methodology and leveraging the powerful automation in Catapult, you can achieve results comparable to hand-tuned RTL in a fraction of the development time. Perhaps more importantly, you gain the ability to iterate and experiment rapidly at a high level, leading to more exploration of the design space and potentially more innovative solutions. As demonstrated, even complex algorithms like neural network functions can be implemented efficiently by combining high-level libraries with HLS, all while maintaining control over hardware-specific details (bit-widths, parallelism, etc.) where it matters.

In ASIC design, where the cost of error is high, Catapult’s verification and formal checking capabilities ensure that high-level designs can be trusted and verified before committing to silicon. The gap between a C++ model and the final chip is now much smaller, with HLS handling the heavy lifting of translation and optimization.

Finally, remember that successful HLS design still requires hardware thinking – understanding the consequences of your coding style on hardware and knowing which knobs to turn (pragmas, directives) to guide the outcome. We encourage you to apply these principles to your own projects. Start simple, validate often, and incrementally add optimizations. With practice, you’ll harness Catapult HLS to deliver high-quality ASIC hardware designs faster and with greater confidence than traditional methods, truly “catapulting” your productivity to the next level.

**References:** High-Level Synthesis concepts and Catapult-specific features referenced in this tutorial are drawn from Siemens EDA documentation and user experiences, reflecting the state-of-the-art in HLS for ASIC design.
